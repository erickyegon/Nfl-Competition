{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ NFL Player Movement - Model Comparison\n",
    "\n",
    "**Comprehensive Model Selection and Ensemble Methods**\n",
    "\n",
    "This notebook compares multiple machine learning models for player movement prediction and explores ensemble techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table of Contents\n",
    "\n",
    "1. [Setup & Configuration](#1-setup)\n",
    "2. [Data Loading & Preparation](#2-data)\n",
    "3. [Train/Validation Split](#3-split)\n",
    "4. [Model Training](#4-training)\n",
    "5. [Cross-Validation](#5-cv)\n",
    "6. [Hyperparameter Tuning](#6-tuning)\n",
    "7. [Model Comparison](#7-comparison)\n",
    "8. [Ensemble Methods](#8-ensemble)\n",
    "9. [Best Model Selection](#9-selection)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration üîß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Try to import XGBoost and LightGBM\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    HAS_XGBOOST = True\n",
    "except ImportError:\n",
    "    HAS_XGBOOST = False\n",
    "    print(\"‚ö†Ô∏è  XGBoost not installed. Install with: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LIGHTGBM = True\n",
    "except ImportError:\n",
    "    HAS_LIGHTGBM = False\n",
    "    print(\"‚ö†Ô∏è  LightGBM not installed. Install with: pip install lightgbm\")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"   XGBoost available: {HAS_XGBOOST}\")\n",
    "print(f\"   LightGBM available: {HAS_LIGHTGBM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    \"\"\"Model comparison configuration\"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    DATA_DIR = Path('../data/raw/train')\n",
    "    OUTPUT_DIR = Path('../outputs/model_comparison')\n",
    "    \n",
    "    # Data settings\n",
    "    USE_SAMPLE = True\n",
    "    SAMPLE_SIZE = 50000\n",
    "    MAX_FILES = 2\n",
    "    \n",
    "    # Split settings\n",
    "    VAL_SIZE = 0.2\n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "    # Cross-validation\n",
    "    N_FOLDS = 5\n",
    "    \n",
    "    # Models to compare\n",
    "    MODELS = ['ridge', 'lasso', 'elastic_net', 'random_forest', 'gradient_boosting']\n",
    "    if HAS_XGBOOST:\n",
    "        MODELS.append('xgboost')\n",
    "    if HAS_LIGHTGBM:\n",
    "        MODELS.append('lightgbm')\n",
    "\n",
    "config = Config()\n",
    "config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Models to compare: {len(config.MODELS)}\")\n",
    "print(f\"   Models: {config.MODELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preparation üìÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(data_dir, max_files=None, sample_size=None):\n",
    "    \"\"\"\n",
    "    Load data and create features\n",
    "    \n",
    "    Returns:\n",
    "        df: Dataframe with features and targets\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Loading data...\\n\")\n",
    "    \n",
    "    # Get file lists\n",
    "    input_files = sorted(data_dir.glob('input_*.csv'))[:max_files] if max_files else sorted(data_dir.glob('input_*.csv'))\n",
    "    output_files = sorted(data_dir.glob('output_*.csv'))[:max_files] if max_files else sorted(data_dir.glob('output_*.csv'))\n",
    "    \n",
    "    # Load\n",
    "    input_df = pd.concat([pd.read_csv(f) for f in input_files], ignore_index=True)\n",
    "    output_df = pd.concat([pd.read_csv(f) for f in output_files], ignore_index=True)\n",
    "    \n",
    "    print(f\"   Input: {input_df.shape}\")\n",
    "    print(f\"   Output: {output_df.shape}\")\n",
    "    \n",
    "    # Sample\n",
    "    if sample_size and len(input_df) > sample_size:\n",
    "        input_df = input_df.sample(n=sample_size, random_state=42)\n",
    "        sampled_keys = input_df[['game_id', 'play_id', 'nfl_id', 'frame_id']]\n",
    "        output_df = output_df.merge(sampled_keys, on=['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    \n",
    "    # Merge\n",
    "    df = input_df.merge(\n",
    "        output_df[['game_id', 'play_id', 'nfl_id', 'frame_id', 'x', 'y']],\n",
    "        on=['game_id', 'play_id', 'nfl_id', 'frame_id'],\n",
    "        suffixes=('', '_target')\n",
    "    )\n",
    "    df = df.rename(columns={'x_target': 'target_x', 'y_target': 'target_y'})\n",
    "    \n",
    "    # Handle missing values\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().any():\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Data loaded and merged: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create all features (physics, spatial, NFL)\n",
    "    \"\"\"\n",
    "    print(\"\\n‚öôÔ∏è  Creating features...\\n\")\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Physics features\n",
    "    if 's' in df_copy.columns and 'dir' in df_copy.columns:\n",
    "        df_copy['velocity_x'] = df_copy['s'] * np.cos(np.radians(df_copy['dir']))\n",
    "        df_copy['velocity_y'] = df_copy['s'] * np.sin(np.radians(df_copy['dir']))\n",
    "        print(\"   ‚úì Physics features\")\n",
    "    \n",
    "    if 'a' in df_copy.columns and 'dir' in df_copy.columns:\n",
    "        df_copy['acceleration_x'] = df_copy['a'] * np.cos(np.radians(df_copy['dir']))\n",
    "        df_copy['acceleration_y'] = df_copy['a'] * np.sin(np.radians(df_copy['dir']))\n",
    "    \n",
    "    if 'player_weight' in df_copy.columns and 's' in df_copy.columns:\n",
    "        df_copy['momentum'] = df_copy['player_weight'] * df_copy['s']\n",
    "        df_copy['kinetic_energy'] = 0.5 * df_copy['player_weight'] * (df_copy['s'] ** 2)\n",
    "    \n",
    "    # Spatial features\n",
    "    if all(col in df_copy.columns for col in ['x', 'y', 'ball_land_x', 'ball_land_y']):\n",
    "        df_copy['dist_to_ball'] = np.sqrt((df_copy['x'] - df_copy['ball_land_x'])**2 + (df_copy['y'] - df_copy['ball_land_y'])**2)\n",
    "        df_copy['dx_to_ball'] = df_copy['ball_land_x'] - df_copy['x']\n",
    "        df_copy['dy_to_ball'] = df_copy['ball_land_y'] - df_copy['y']\n",
    "        print(\"   ‚úì Spatial features\")\n",
    "    \n",
    "    if 'x' in df_copy.columns:\n",
    "        df_copy['field_position_norm'] = df_copy['x'] / 120.0\n",
    "        df_copy['dist_to_sideline'] = np.minimum(df_copy['y'], 53.3 - df_copy['y'])\n",
    "    \n",
    "    # NFL features\n",
    "    if 'player_role' in df_copy.columns:\n",
    "        df_copy['is_targeted_receiver'] = (df_copy['player_role'] == 'Targeted Receiver').astype(int)\n",
    "        df_copy['is_passer'] = (df_copy['player_role'] == 'Passer').astype(int)\n",
    "        print(\"   ‚úì NFL domain features\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Features created: {len([c for c in df_copy.columns if c not in df.columns])} new\")\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "df = load_and_prepare_data(\n",
    "    config.DATA_DIR,\n",
    "    max_files=config.MAX_FILES,\n",
    "    sample_size=config.SAMPLE_SIZE if config.USE_SAMPLE else None\n",
    ")\n",
    "\n",
    "df = create_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Validation Split üîÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_data(df, val_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare features and split into train/validation\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_val, y_train_x, y_val_x, y_train_y, y_val_y, feature_names\n",
    "    \"\"\"\n",
    "    print(\"\\nüîÄ Preparing model data...\\n\")\n",
    "    \n",
    "    # Identify feature columns\n",
    "    exclude_cols = ['game_id', 'play_id', 'nfl_id', 'frame_id', 'target_x', 'target_y',\n",
    "                    'player_name', 'player_position', 'player_role', 'player_side',\n",
    "                    'play_direction', 'player_birth_date', 'player_to_predict']\n",
    "    \n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols and df[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    # Prepare features and targets\n",
    "    X = df[feature_cols].fillna(0)\n",
    "    y_x = df['target_x'].fillna(df['target_x'].median())\n",
    "    y_y = df['target_y'].fillna(df['target_y'].median())\n",
    "    \n",
    "    # Temporal split by game (if available)\n",
    "    if 'game_id' in df.columns:\n",
    "        unique_games = df['game_id'].unique()\n",
    "        np.random.seed(random_state)\n",
    "        np.random.shuffle(unique_games)\n",
    "        \n",
    "        split_idx = int(len(unique_games) * (1 - val_size))\n",
    "        train_games = unique_games[:split_idx]\n",
    "        val_games = unique_games[split_idx:]\n",
    "        \n",
    "        train_mask = df['game_id'].isin(train_games)\n",
    "        val_mask = df['game_id'].isin(val_games)\n",
    "        \n",
    "        X_train, X_val = X[train_mask], X[val_mask]\n",
    "        y_train_x, y_val_x = y_x[train_mask], y_x[val_mask]\n",
    "        y_train_y, y_val_y = y_y[train_mask], y_y[val_mask]\n",
    "        \n",
    "        print(\"   Split type: Temporal (by game)\")\n",
    "    else:\n",
    "        # Random split\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_val, y_train_x, y_val_x = train_test_split(X, y_x, test_size=val_size, random_state=random_state)\n",
    "        _, _, y_train_y, y_val_y = train_test_split(X, y_y, test_size=val_size, random_state=random_state)\n",
    "        print(\"   Split type: Random\")\n",
    "    \n",
    "    print(f\"   Train: {X_train.shape[0]:,} samples\")\n",
    "    print(f\"   Val: {X_val.shape[0]:,} samples\")\n",
    "    print(f\"   Features: {X_train.shape[1]}\")\n",
    "    \n",
    "    return X_train, X_val, y_train_x, y_val_x, y_train_y, y_val_y, feature_cols\n",
    "\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train_x, y_val_x, y_train_y, y_val_y, feature_names = prepare_model_data(\n",
    "    df, val_size=config.VAL_SIZE, random_state=config.RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Data prepared for modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training ü§ñ\n",
    "\n",
    "Train multiple models and compare performance:\n",
    "- **Linear Models**: Ridge, Lasso, ElasticNet\n",
    "- **Tree Models**: Random Forest, Gradient Boosting\n",
    "- **Boosting Models**: XGBoost, LightGBM (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    \"\"\"\n",
    "    Get model instance by name\n",
    "    \"\"\"\n",
    "    if model_name == 'ridge':\n",
    "        return Ridge(alpha=1.0, random_state=42)\n",
    "    elif model_name == 'lasso':\n",
    "        return Lasso(alpha=1.0, random_state=42, max_iter=2000)\n",
    "    elif model_name == 'elastic_net':\n",
    "        return ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42, max_iter=2000)\n",
    "    elif model_name == 'random_forest':\n",
    "        return RandomForestRegressor(n_estimators=100, max_depth=15, min_samples_split=10, random_state=42, n_jobs=-1)\n",
    "    elif model_name == 'gradient_boosting':\n",
    "        return GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "    elif model_name == 'xgboost' and HAS_XGBOOST:\n",
    "        return XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    "    elif model_name == 'lightgbm' and HAS_LIGHTGBM:\n",
    "        return lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, n_jobs=-1, verbose=-1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    \"\"\"\n",
    "    Train model and evaluate performance\n",
    "    \n",
    "    Returns:\n",
    "        model, metrics, predictions, training_time\n",
    "    \"\"\"\n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "        'val_rmse': np.sqrt(mean_squared_error(y_val, y_pred_val)),\n",
    "        'train_mae': mean_absolute_error(y_train, y_pred_train),\n",
    "        'val_mae': mean_absolute_error(y_val, y_pred_val),\n",
    "        'train_r2': r2_score(y_train, y_pred_train),\n",
    "        'val_r2': r2_score(y_val, y_pred_val),\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    return model, metrics, y_pred_val, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models for X coordinate\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING MODELS - X COORDINATE\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "models_x = {}\n",
    "metrics_x = []\n",
    "predictions_x = {}\n",
    "\n",
    "for model_name in config.MODELS:\n",
    "    print(f\"ü§ñ Training {model_name.upper()}...\")\n",
    "    model = get_model(model_name)\n",
    "    model, metrics, pred, train_time = train_and_evaluate(model, X_train, y_train_x, X_val, y_val_x, model_name)\n",
    "    \n",
    "    models_x[model_name] = model\n",
    "    metrics_x.append(metrics)\n",
    "    predictions_x[model_name] = pred\n",
    "    \n",
    "    print(f\"   ‚úì Val RMSE: {metrics['val_rmse']:.4f} | Val R¬≤: {metrics['val_r2']:.4f} | Time: {train_time:.2f}s\\n\")\n",
    "\n",
    "print(\"‚úÖ X coordinate models trained\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models for Y coordinate\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING MODELS - Y COORDINATE\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "models_y = {}\n",
    "metrics_y = []\n",
    "predictions_y = {}\n",
    "\n",
    "for model_name in config.MODELS:\n",
    "    print(f\"ü§ñ Training {model_name.upper()}...\")\n",
    "    model = get_model(model_name)\n",
    "    model, metrics, pred, train_time = train_and_evaluate(model, X_train, y_train_y, X_val, y_val_y, model_name)\n",
    "    \n",
    "    models_y[model_name] = model\n",
    "    metrics_y.append(metrics)\n",
    "    predictions_y[model_name] = pred\n",
    "    \n",
    "    print(f\"   ‚úì Val RMSE: {metrics['val_rmse']:.4f} | Val R¬≤: {metrics['val_r2']:.4f} | Time: {train_time:.2f}s\\n\")\n",
    "\n",
    "print(\"‚úÖ Y coordinate models trained\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation üîÑ\n",
    "\n",
    "Perform K-Fold and Time-Series cross-validation for robust evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(model_name, X, y, cv_type='kfold', n_splits=5):\n",
    "    \"\"\"\n",
    "    Perform cross-validation\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of model\n",
    "        X, y: Features and target\n",
    "        cv_type: 'kfold' or 'timeseries'\n",
    "        n_splits: Number of folds\n",
    "    \n",
    "    Returns:\n",
    "        cv_scores: List of RMSE scores for each fold\n",
    "    \"\"\"\n",
    "    # Select CV strategy\n",
    "    if cv_type == 'kfold':\n",
    "        cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    else:\n",
    "        cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X), 1):\n",
    "        X_train_cv = X.iloc[train_idx]\n",
    "        X_val_cv = X.iloc[val_idx]\n",
    "        y_train_cv = y.iloc[train_idx]\n",
    "        y_val_cv = y.iloc[val_idx]\n",
    "        \n",
    "        # Train and evaluate\n",
    "        model = get_model(model_name)\n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "        y_pred = model.predict(X_val_cv)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_cv, y_pred))\n",
    "        cv_scores.append(rmse)\n",
    "    \n",
    "    return cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Fold CV for top 3 models (faster)\n",
    "print(\"üîÑ Performing K-Fold Cross-Validation...\\n\")\n",
    "\n",
    "# Select top 3 models based on validation RMSE\n",
    "top_3_models = sorted(metrics_x, key=lambda x: x['val_rmse'])[:3]\n",
    "top_3_names = [m['model'] for m in top_3_models]\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for model_name in top_3_names:\n",
    "    print(f\"   {model_name.upper()}...\")\n",
    "    cv_scores = cross_validate_model(model_name, X_train, y_train_x, cv_type='kfold', n_splits=config.N_FOLDS)\n",
    "    cv_results[model_name] = cv_scores\n",
    "    print(f\"      Mean RMSE: {np.mean(cv_scores):.4f} ¬± {np.std(cv_scores):.4f}\\n\")\n",
    "\n",
    "print(\"‚úÖ Cross-validation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV results\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "cv_data = [cv_results[name] for name in top_3_names]\n",
    "positions = range(1, len(top_3_names) + 1)\n",
    "\n",
    "bp = ax.boxplot(cv_data, positions=positions, labels=[n.upper() for n in top_3_names],\n",
    "                patch_artist=True, widths=0.6)\n",
    "\n",
    "# Color boxes\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax.set_ylabel('RMSE', fontsize=12)\n",
    "ax.set_title(f'{config.N_FOLDS}-Fold Cross-Validation Results', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.OUTPUT_DIR / 'cross_validation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Cross-validation visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning üéõÔ∏è\n",
    "\n",
    "Perform grid search for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Tune Random Forest\n",
    "print(\"üéõÔ∏è  Tuning Random Forest hyperparameters...\\n\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [10, 20]\n",
    "}\n",
    "\n",
    "# Use smaller sample for faster tuning\n",
    "sample_size = min(10000, len(X_train))\n",
    "sample_idx = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "X_tune = X_train.iloc[sample_idx]\n",
    "y_tune = y_train_x.iloc[sample_idx]\n",
    "\n",
    "rf_base = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "grid_search = GridSearchCV(rf_base, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_tune, y_tune)\n",
    "\n",
    "print(f\"\\n‚úÖ Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"   Best RMSE: {np.sqrt(-grid_search.best_score_):.4f}\")\n",
    "\n",
    "# Save tuning results\n",
    "tuning_results = pd.DataFrame(grid_search.cv_results_)\n",
    "tuning_results.to_csv(config.OUTPUT_DIR / 'hyperparameter_tuning.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison üìä\n",
    "\n",
    "Compare all models across multiple metrics and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(metrics_x)\n",
    "comparison_df_y = pd.DataFrame(metrics_y)\n",
    "\n",
    "# Merge X and Y metrics\n",
    "comparison = pd.merge(\n",
    "    comparison_df[['model', 'val_rmse', 'val_mae', 'val_r2', 'training_time']],\n",
    "    comparison_df_y[['model', 'val_rmse', 'val_mae', 'val_r2']],\n",
    "    on='model',\n",
    "    suffixes=('_x', '_y')\n",
    ")\n",
    "\n",
    "# Add average RMSE\n",
    "comparison['avg_rmse'] = (comparison['val_rmse_x'] + comparison['val_rmse_y']) / 2\n",
    "comparison = comparison.sort_values('avg_rmse')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "display(comparison)\n",
    "\n",
    "# Save comparison\n",
    "comparison.to_csv(config.OUTPUT_DIR / 'model_comparison.csv', index=False)\n",
    "print(f\"\\n‚úÖ Comparison saved to: {config.OUTPUT_DIR / 'model_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "models_list = comparison['model'].tolist()\n",
    "x_pos = np.arange(len(models_list))\n",
    "\n",
    "# 1. RMSE comparison\n",
    "axes[0, 0].bar(x_pos - 0.2, comparison['val_rmse_x'], width=0.4, label='X coordinate', alpha=0.8)\n",
    "axes[0, 0].bar(x_pos + 0.2, comparison['val_rmse_y'], width=0.4, label='Y coordinate', alpha=0.8)\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels([m.upper() for m in models_list], rotation=45, ha='right')\n",
    "axes[0, 0].set_ylabel('RMSE', fontsize=12)\n",
    "axes[0, 0].set_title('Model RMSE Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. MAE comparison\n",
    "axes[0, 1].bar(x_pos - 0.2, comparison['val_mae_x'], width=0.4, label='X coordinate', alpha=0.8, color='orange')\n",
    "axes[0, 1].bar(x_pos + 0.2, comparison['val_mae_y'], width=0.4, label='Y coordinate', alpha=0.8, color='red')\n",
    "axes[0, 1].set_xticks(x_pos)\n",
    "axes[0, 1].set_xticklabels([m.upper() for m in models_list], rotation=45, ha='right')\n",
    "axes[0, 1].set_ylabel('MAE', fontsize=12)\n",
    "axes[0, 1].set_title('Model MAE Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. R¬≤ comparison\n",
    "axes[1, 0].bar(x_pos - 0.2, comparison['val_r2_x'], width=0.4, label='X coordinate', alpha=0.8, color='green')\n",
    "axes[1, 0].bar(x_pos + 0.2, comparison['val_r2_y'], width=0.4, label='Y coordinate', alpha=0.8, color='cyan')\n",
    "axes[1, 0].set_xticks(x_pos)\n",
    "axes[1, 0].set_xticklabels([m.upper() for m in models_list], rotation=45, ha='right')\n",
    "axes[1, 0].set_ylabel('R¬≤ Score', fontsize=12)\n",
    "axes[1, 0].set_title('Model R¬≤ Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Training time\n",
    "axes[1, 1].bar(x_pos, comparison['training_time'], alpha=0.8, color='purple')\n",
    "axes[1, 1].set_xticks(x_pos)\n",
    "axes[1, 1].set_xticklabels([m.upper() for m in models_list], rotation=45, ha='right')\n",
    "axes[1, 1].set_ylabel('Training Time (seconds)', fontsize=12)\n",
    "axes[1, 1].set_title('Model Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.OUTPUT_DIR / 'model_comparison_charts.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Model comparison visualized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance comparison (for tree-based models)\n",
    "print(\"\\nüå≥ Comparing feature importance across tree-based models...\\n\")\n",
    "\n",
    "tree_models = ['random_forest', 'gradient_boosting']\n",
    "if HAS_XGBOOST:\n",
    "    tree_models.append('xgboost')\n",
    "if HAS_LIGHTGBM:\n",
    "    tree_models.append('lightgbm')\n",
    "\n",
    "# Get available tree models\n",
    "available_tree_models = [m for m in tree_models if m in models_x]\n",
    "\n",
    "if available_tree_models:\n",
    "    fig, axes = plt.subplots(1, len(available_tree_models), figsize=(6*len(available_tree_models), 6))\n",
    "    if len(available_tree_models) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, model_name in enumerate(available_tree_models):\n",
    "        model = models_x[model_name]\n",
    "        \n",
    "        # Get feature importances\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            top_n = 15\n",
    "            indices = np.argsort(importances)[-top_n:]\n",
    "            \n",
    "            axes[idx].barh(range(top_n), importances[indices], alpha=0.8)\n",
    "            axes[idx].set_yticks(range(top_n))\n",
    "            axes[idx].set_yticklabels([feature_names[i] for i in indices], fontsize=9)\n",
    "            axes[idx].set_xlabel('Importance', fontsize=11)\n",
    "            axes[idx].set_title(f'{model_name.upper()}\\nTop 15 Features', fontsize=12, fontweight='bold')\n",
    "            axes[idx].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.OUTPUT_DIR / 'feature_importance_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úÖ Feature importance compared\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No tree-based models available for feature importance comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ensemble Methods üé≠\n",
    "\n",
    "Combine multiple models using ensemble techniques:\n",
    "- Simple averaging\n",
    "- Weighted averaging\n",
    "- Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé≠ Creating ensemble predictions...\\n\")\n",
    "\n",
    "# 1. Simple averaging ensemble\n",
    "ensemble_simple_x = np.mean([predictions_x[m] for m in config.MODELS], axis=0)\n",
    "ensemble_simple_y = np.mean([predictions_y[m] for m in config.MODELS], axis=0)\n",
    "\n",
    "rmse_ensemble_simple_x = np.sqrt(mean_squared_error(y_val_x, ensemble_simple_x))\n",
    "rmse_ensemble_simple_y = np.sqrt(mean_squared_error(y_val_y, ensemble_simple_y))\n",
    "\n",
    "print(f\"üìä Simple Averaging Ensemble:\")\n",
    "print(f\"   X RMSE: {rmse_ensemble_simple_x:.4f}\")\n",
    "print(f\"   Y RMSE: {rmse_ensemble_simple_y:.4f}\")\n",
    "print(f\"   Average: {(rmse_ensemble_simple_x + rmse_ensemble_simple_y) / 2:.4f}\\n\")\n",
    "\n",
    "# 2. Weighted averaging (weight by inverse RMSE)\n",
    "weights_x = np.array([1 / m['val_rmse'] for m in metrics_x])\n",
    "weights_x = weights_x / weights_x.sum()\n",
    "\n",
    "weights_y = np.array([1 / m['val_rmse'] for m in metrics_y])\n",
    "weights_y = weights_y / weights_y.sum()\n",
    "\n",
    "ensemble_weighted_x = np.sum([w * predictions_x[m] for w, m in zip(weights_x, config.MODELS)], axis=0)\n",
    "ensemble_weighted_y = np.sum([w * predictions_y[m] for w, m in zip(weights_y, config.MODELS)], axis=0)\n",
    "\n",
    "rmse_ensemble_weighted_x = np.sqrt(mean_squared_error(y_val_x, ensemble_weighted_x))\n",
    "rmse_ensemble_weighted_y = np.sqrt(mean_squared_error(y_val_y, ensemble_weighted_y))\n",
    "\n",
    "print(f\"üìä Weighted Averaging Ensemble:\")\n",
    "print(f\"   X RMSE: {rmse_ensemble_weighted_x:.4f}\")\n",
    "print(f\"   Y RMSE: {rmse_ensemble_weighted_y:.4f}\")\n",
    "print(f\"   Average: {(rmse_ensemble_weighted_x + rmse_ensemble_weighted_y) / 2:.4f}\\n\")\n",
    "\n",
    "print(f\"   Weights X: {dict(zip(config.MODELS, [f'{w:.3f}' for w in weights_x]))}\")\n",
    "print(f\"   Weights Y: {dict(zip(config.MODELS, [f'{w:.3f}' for w in weights_y]))}\\n\")\n",
    "\n",
    "# 3. Stacking (use Ridge as meta-learner)\n",
    "print(\"üèóÔ∏è  Training stacking ensemble...\")\n",
    "\n",
    "# Prepare meta-features (predictions from base models)\n",
    "meta_features_x = np.column_stack([predictions_x[m] for m in config.MODELS])\n",
    "meta_features_y = np.column_stack([predictions_y[m] for m in config.MODELS])\n",
    "\n",
    "# Train meta-learner\n",
    "meta_learner_x = Ridge(alpha=1.0, random_state=42)\n",
    "meta_learner_y = Ridge(alpha=1.0, random_state=42)\n",
    "\n",
    "meta_learner_x.fit(meta_features_x, y_val_x)\n",
    "meta_learner_y.fit(meta_features_y, y_val_y)\n",
    "\n",
    "ensemble_stacking_x = meta_learner_x.predict(meta_features_x)\n",
    "ensemble_stacking_y = meta_learner_y.predict(meta_features_y)\n",
    "\n",
    "rmse_ensemble_stacking_x = np.sqrt(mean_squared_error(y_val_x, ensemble_stacking_x))\n",
    "rmse_ensemble_stacking_y = np.sqrt(mean_squared_error(y_val_y, ensemble_stacking_y))\n",
    "\n",
    "print(f\"\\nüìä Stacking Ensemble (Ridge meta-learner):\")\n",
    "print(f\"   X RMSE: {rmse_ensemble_stacking_x:.4f}\")\n",
    "print(f\"   Y RMSE: {rmse_ensemble_stacking_y:.4f}\")\n",
    "print(f\"   Average: {(rmse_ensemble_stacking_x + rmse_ensemble_stacking_y) / 2:.4f}\\n\")\n",
    "\n",
    "print(\"‚úÖ Ensemble methods created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ensemble comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Add ensemble results to comparison\n",
    "ensemble_results = [\n",
    "    ('Simple Avg', (rmse_ensemble_simple_x + rmse_ensemble_simple_y) / 2),\n",
    "    ('Weighted Avg', (rmse_ensemble_weighted_x + rmse_ensemble_weighted_y) / 2),\n",
    "    ('Stacking', (rmse_ensemble_stacking_x + rmse_ensemble_stacking_y) / 2)\n",
    "]\n",
    "\n",
    "# X coordinate comparison\n",
    "individual_rmse_x = comparison['val_rmse_x'].tolist()\n",
    "ensemble_rmse_x = [rmse_ensemble_simple_x, rmse_ensemble_weighted_x, rmse_ensemble_stacking_x]\n",
    "\n",
    "all_models_x = [m.upper() for m in models_list] + ['Simple\\nAvg', 'Weighted\\nAvg', 'Stacking']\n",
    "all_rmse_x = individual_rmse_x + ensemble_rmse_x\n",
    "\n",
    "colors_x = ['steelblue'] * len(individual_rmse_x) + ['green', 'orange', 'red']\n",
    "axes[0].bar(range(len(all_models_x)), all_rmse_x, color=colors_x, alpha=0.7)\n",
    "axes[0].set_xticks(range(len(all_models_x)))\n",
    "axes[0].set_xticklabels(all_models_x, rotation=45, ha='right')\n",
    "axes[0].set_ylabel('RMSE', fontsize=12)\n",
    "axes[0].set_title('X Coordinate: Individual vs Ensemble Models', fontsize=14, fontweight='bold')\n",
    "axes[0].axhline(min(individual_rmse_x), color='red', linestyle='--', linewidth=1, label='Best individual')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Y coordinate comparison\n",
    "individual_rmse_y = comparison['val_rmse_y'].tolist()\n",
    "ensemble_rmse_y = [rmse_ensemble_simple_y, rmse_ensemble_weighted_y, rmse_ensemble_stacking_y]\n",
    "\n",
    "all_rmse_y = individual_rmse_y + ensemble_rmse_y\n",
    "\n",
    "colors_y = ['steelblue'] * len(individual_rmse_y) + ['green', 'orange', 'red']\n",
    "axes[1].bar(range(len(all_models_x)), all_rmse_y, color=colors_y, alpha=0.7)\n",
    "axes[1].set_xticks(range(len(all_models_x)))\n",
    "axes[1].set_xticklabels(all_models_x, rotation=45, ha='right')\n",
    "axes[1].set_ylabel('RMSE', fontsize=12)\n",
    "axes[1].set_title('Y Coordinate: Individual vs Ensemble Models', fontsize=14, fontweight='bold')\n",
    "axes[1].axhline(min(individual_rmse_y), color='red', linestyle='--', linewidth=1, label='Best individual')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.OUTPUT_DIR / 'ensemble_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Ensemble comparison visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Model Selection üèÜ\n",
    "\n",
    "Select the best model and save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best individual models\n",
    "best_model_x = comparison.iloc[0]['model']\n",
    "best_rmse_x = comparison.iloc[0]['val_rmse_x']\n",
    "\n",
    "best_model_y_idx = comparison['val_rmse_y'].idxmin()\n",
    "best_model_y = comparison.iloc[best_model_y_idx]['model']\n",
    "best_rmse_y = comparison.iloc[best_model_y_idx]['val_rmse_y']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BEST MODEL SELECTION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"üèÜ Best Individual Models:\")\n",
    "print(f\"   X coordinate: {best_model_x.upper()} (RMSE: {best_rmse_x:.4f})\")\n",
    "print(f\"   Y coordinate: {best_model_y.upper()} (RMSE: {best_rmse_y:.4f})\")\n",
    "print(f\"   Average: {(best_rmse_x + best_rmse_y) / 2:.4f}\\n\")\n",
    "\n",
    "print(\"üé≠ Best Ensemble:\")\n",
    "ensemble_avg_rmse = [\n",
    "    ('Simple Averaging', (rmse_ensemble_simple_x + rmse_ensemble_simple_y) / 2),\n",
    "    ('Weighted Averaging', (rmse_ensemble_weighted_x + rmse_ensemble_weighted_y) / 2),\n",
    "    ('Stacking', (rmse_ensemble_stacking_x + rmse_ensemble_stacking_y) / 2)\n",
    "]\n",
    "best_ensemble = min(ensemble_avg_rmse, key=lambda x: x[1])\n",
    "print(f\"   {best_ensemble[0]} (Avg RMSE: {best_ensemble[1]:.4f})\\n\")\n",
    "\n",
    "# Overall recommendation\n",
    "all_options = [\n",
    "    (f\"Individual: {best_model_x}\", (best_rmse_x + best_rmse_y) / 2),\n",
    "    (f\"Ensemble: {best_ensemble[0]}\", best_ensemble[1])\n",
    "]\n",
    "overall_best = min(all_options, key=lambda x: x[1])\n",
    "\n",
    "print(\"üéØ Recommendation:\")\n",
    "print(f\"   Use: {overall_best[0]}\")\n",
    "print(f\"   Expected RMSE: {overall_best[1]:.4f}\\n\")\n",
    "\n",
    "# Save best models\n",
    "print(\"üíæ Saving best models...\")\n",
    "with open(config.OUTPUT_DIR / f'best_model_x_{best_model_x}.pkl', 'wb') as f:\n",
    "    pickle.dump(models_x[best_model_x], f)\n",
    "\n",
    "with open(config.OUTPUT_DIR / f'best_model_y_{best_model_y}.pkl', 'wb') as f:\n",
    "    pickle.dump(models_y[best_model_y], f)\n",
    "\n",
    "# Save feature names\n",
    "with open(config.OUTPUT_DIR / 'feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_names, f)\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'best_model_x': best_model_x,\n",
    "    'best_model_y': best_model_y,\n",
    "    'best_rmse_x': float(best_rmse_x),\n",
    "    'best_rmse_y': float(best_rmse_y),\n",
    "    'best_ensemble': best_ensemble[0],\n",
    "    'best_ensemble_rmse': float(best_ensemble[1]),\n",
    "    'overall_recommendation': overall_best[0],\n",
    "    'expected_rmse': float(overall_best[1]),\n",
    "    'num_models_compared': len(config.MODELS),\n",
    "    'num_features': len(feature_names)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(config.OUTPUT_DIR / 'summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {config.OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Model Comparison Complete!\n",
    "\n",
    "### Summary:\n",
    "\n",
    "‚úÖ **Models Compared**: Linear (Ridge, Lasso, ElasticNet), Tree-based (Random Forest, Gradient Boosting), Boosting (XGBoost, LightGBM)  \n",
    "‚úÖ **Cross-Validation**: K-Fold CV performed on top models  \n",
    "‚úÖ **Hyperparameter Tuning**: Grid search for Random Forest  \n",
    "‚úÖ **Ensemble Methods**: Simple averaging, weighted averaging, stacking  \n",
    "‚úÖ **Best Model Selected**: Based on validation RMSE  \n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Best Individual Model**: Typically tree-based models (Random Forest, XGBoost, LightGBM)\n",
    "2. **Ensemble Performance**: Often improves over individual models\n",
    "3. **Trade-offs**: Linear models are fast but less accurate; tree models are slower but more accurate\n",
    "4. **Feature Importance**: Consistent across tree-based models\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Try LSTM models in `05_lstm_sequence_modeling.ipynb`\n",
    "2. Generate final predictions in `06_prediction_and_evaluation.ipynb`\n",
    "3. Experiment with feature selection to improve speed\n",
    "4. Try more advanced ensembles (blending, stacking with different meta-learners)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

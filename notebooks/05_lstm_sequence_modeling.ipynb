{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† NFL Player Movement - LSTM Sequence Modeling\n",
    "\n",
    "**Deep Learning for Trajectory Prediction using LSTM Networks**\n",
    "\n",
    "This notebook explores LSTM (Long Short-Term Memory) networks for predicting player trajectories using temporal sequences.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table of Contents\n",
    "\n",
    "1. [Setup & Configuration](#1-setup)\n",
    "2. [Data Loading](#2-data)\n",
    "3. [Sequence Creation](#3-sequences)\n",
    "4. [LSTM Architecture](#4-architecture)\n",
    "5. [Model Training](#5-training)\n",
    "6. [Predictions & Visualization](#6-predictions)\n",
    "7. [Comparison with Traditional Models](#7-comparison)\n",
    "8. [Error Analysis](#8-analysis)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration üîß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "    HAS_PYTORCH = True\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__} available\")\n",
    "    print(f\"   Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "except ImportError:\n",
    "    HAS_PYTORCH = False\n",
    "    print(\"‚ö†Ô∏è  PyTorch not installed. Install with: pip install torch\")\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"\\n‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    \"\"\"LSTM modeling configuration\"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    DATA_DIR = Path('../data/raw/train')\n",
    "    OUTPUT_DIR = Path('../outputs/lstm_modeling')\n",
    "    \n",
    "    # Data settings\n",
    "    USE_SAMPLE = True\n",
    "    SAMPLE_SIZE = 30000\n",
    "    MAX_FILES = 2\n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "    # Sequence settings\n",
    "    SEQUENCE_LENGTH = 10  # Number of frames to look back\n",
    "    \n",
    "    # Model settings\n",
    "    HIDDEN_SIZE = 64\n",
    "    NUM_LAYERS = 2\n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    # Training settings\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCHS = 20\n",
    "    LEARNING_RATE = 0.001\n",
    "    EARLY_STOPPING_PATIENCE = 5\n",
    "    \n",
    "    # Device\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() and HAS_PYTORCH else 'cpu')\n",
    "\n",
    "config = Config()\n",
    "config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Sequence length: {config.SEQUENCE_LENGTH} frames\")\n",
    "print(f\"   Device: {config.DEVICE}\")\n",
    "print(f\"   Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"   Epochs: {config.EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading üìÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(data_dir, max_files=None, sample_size=None):\n",
    "    \"\"\"\n",
    "    Load and prepare data for sequence modeling\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Loading data...\\n\")\n",
    "    \n",
    "    # Load files\n",
    "    input_files = sorted(data_dir.glob('input_*.csv'))[:max_files] if max_files else sorted(data_dir.glob('input_*.csv'))\n",
    "    output_files = sorted(data_dir.glob('output_*.csv'))[:max_files] if max_files else sorted(data_dir.glob('output_*.csv'))\n",
    "    \n",
    "    input_df = pd.concat([pd.read_csv(f) for f in input_files], ignore_index=True)\n",
    "    output_df = pd.concat([pd.read_csv(f) for f in output_files], ignore_index=True)\n",
    "    \n",
    "    print(f\"   Input: {input_df.shape}\")\n",
    "    print(f\"   Output: {output_df.shape}\")\n",
    "    \n",
    "    # Sample by games (to keep sequences intact)\n",
    "    if sample_size and len(input_df) > sample_size:\n",
    "        unique_games = input_df['game_id'].unique()\n",
    "        np.random.seed(42)\n",
    "        n_games = int(len(unique_games) * (sample_size / len(input_df)))\n",
    "        sampled_games = np.random.choice(unique_games, n_games, replace=False)\n",
    "        input_df = input_df[input_df['game_id'].isin(sampled_games)]\n",
    "        sampled_keys = input_df[['game_id', 'play_id', 'nfl_id', 'frame_id']]\n",
    "        output_df = output_df.merge(sampled_keys, on=['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    \n",
    "    # Merge\n",
    "    df = input_df.merge(\n",
    "        output_df[['game_id', 'play_id', 'nfl_id', 'frame_id', 'x', 'y']],\n",
    "        on=['game_id', 'play_id', 'nfl_id', 'frame_id'],\n",
    "        suffixes=('', '_target')\n",
    "    )\n",
    "    df = df.rename(columns={'x_target': 'target_x', 'y_target': 'target_y'})\n",
    "    \n",
    "    # Sort for sequence creation\n",
    "    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id']).reset_index(drop=True)\n",
    "    \n",
    "    # Handle missing values\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().any():\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Data loaded: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = load_and_prepare_data(\n",
    "    config.DATA_DIR,\n",
    "    max_files=config.MAX_FILES,\n",
    "    sample_size=config.SAMPLE_SIZE if config.USE_SAMPLE else None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sequence Creation üìä\n",
    "\n",
    "Create temporal sequences from tracking data. Each sequence contains the last N frames for a player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Create sequences from tracking data\n",
    "    \n",
    "    For each player at each frame, create a sequence of the previous N frames.\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe with tracking data\n",
    "        sequence_length: Number of frames to include in each sequence\n",
    "    \n",
    "    Returns:\n",
    "        sequences: Array of shape (n_samples, sequence_length, n_features)\n",
    "        targets: Array of shape (n_samples, 2) for (x, y) targets\n",
    "        valid_indices: Indices of valid sequences\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîÑ Creating sequences (length={sequence_length})...\\n\")\n",
    "    \n",
    "    # Select features for sequences\n",
    "    feature_cols = ['x', 'y', 's', 'a', 'dir', 'o']\n",
    "    available_features = [col for col in feature_cols if col in df.columns]\n",
    "    \n",
    "    print(f\"   Features: {available_features}\")\n",
    "    \n",
    "    sequences = []\n",
    "    targets_x = []\n",
    "    targets_y = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    # Group by player within each play\n",
    "    grouped = df.groupby(['game_id', 'play_id', 'nfl_id'])\n",
    "    \n",
    "    for (game_id, play_id, nfl_id), group in grouped:\n",
    "        group = group.sort_values('frame_id').reset_index(drop=True)\n",
    "        \n",
    "        # Create sequences\n",
    "        for i in range(sequence_length, len(group)):\n",
    "            # Extract sequence\n",
    "            seq = group.iloc[i-sequence_length:i][available_features].values\n",
    "            \n",
    "            # Target is the future position at frame i\n",
    "            target_x = group.iloc[i]['target_x']\n",
    "            target_y = group.iloc[i]['target_y']\n",
    "            \n",
    "            if seq.shape[0] == sequence_length and not np.isnan(target_x) and not np.isnan(target_y):\n",
    "                sequences.append(seq)\n",
    "                targets_x.append(target_x)\n",
    "                targets_y.append(target_y)\n",
    "                valid_indices.append(group.iloc[i].name)\n",
    "    \n",
    "    sequences = np.array(sequences, dtype=np.float32)\n",
    "    targets_x = np.array(targets_x, dtype=np.float32)\n",
    "    targets_y = np.array(targets_y, dtype=np.float32)\n",
    "    targets = np.stack([targets_x, targets_y], axis=1)\n",
    "    \n",
    "    print(f\"   ‚úì Sequences created: {sequences.shape}\")\n",
    "    print(f\"   ‚úì Targets: {targets.shape}\")\n",
    "    print(f\"   ‚úì Features per frame: {sequences.shape[2]}\")\n",
    "    \n",
    "    return sequences, targets, valid_indices, available_features\n",
    "\n",
    "\n",
    "# Create sequences\n",
    "sequences, targets, valid_indices, feature_names = create_sequences(df, sequence_length=config.SEQUENCE_LENGTH)\n",
    "\n",
    "print(f\"\\n‚úÖ Sequence creation complete\")\n",
    "print(f\"   Total sequences: {len(sequences):,}\")\n",
    "print(f\"   Sequence shape: {sequences.shape}\")\n",
    "print(f\"   Target shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize sequences\n",
    "print(\"\\nüìè Normalizing sequences...\\n\")\n",
    "\n",
    "# Reshape for scaling\n",
    "n_samples, seq_len, n_features = sequences.shape\n",
    "sequences_reshaped = sequences.reshape(-1, n_features)\n",
    "\n",
    "# Fit scaler\n",
    "scaler = StandardScaler()\n",
    "sequences_normalized = scaler.fit_transform(sequences_reshaped)\n",
    "sequences_normalized = sequences_normalized.reshape(n_samples, seq_len, n_features)\n",
    "\n",
    "print(f\"   ‚úì Sequences normalized\")\n",
    "print(f\"   ‚úì Mean: {sequences_normalized.mean():.4f}\")\n",
    "print(f\"   ‚úì Std: {sequences_normalized.std():.4f}\")\n",
    "\n",
    "# Train/val split (80/20)\n",
    "split_idx = int(0.8 * len(sequences_normalized))\n",
    "\n",
    "X_train = sequences_normalized[:split_idx]\n",
    "X_val = sequences_normalized[split_idx:]\n",
    "y_train = targets[:split_idx]\n",
    "y_val = targets[split_idx:]\n",
    "\n",
    "print(f\"\\n‚úÖ Train/Val split:\")\n",
    "print(f\"   Train: {X_train.shape[0]:,} sequences\")\n",
    "print(f\"   Val: {X_val.shape[0]:,} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM Architecture üèóÔ∏è\n",
    "\n",
    "Define the LSTM neural network architecture for trajectory prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PYTORCH:\n",
    "    class PlayerLSTM(nn.Module):\n",
    "        \"\"\"\n",
    "        LSTM model for player trajectory prediction\n",
    "        \n",
    "        Architecture:\n",
    "        - Input: (batch, sequence_length, n_features)\n",
    "        - LSTM layers with dropout\n",
    "        - Fully connected layer\n",
    "        - Output: (batch, 2) for (x, y) coordinates\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "            super(PlayerLSTM, self).__init__()\n",
    "            \n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "            \n",
    "            # LSTM layers\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout if num_layers > 1 else 0\n",
    "            )\n",
    "            \n",
    "            # Dropout\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            \n",
    "            # Fully connected layer\n",
    "            self.fc = nn.Linear(hidden_size, 2)  # Output: (x, y)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            Forward pass\n",
    "            \n",
    "            Args:\n",
    "                x: Input tensor (batch, sequence_length, input_size)\n",
    "            \n",
    "            Returns:\n",
    "                output: Predicted (x, y) coordinates (batch, 2)\n",
    "            \"\"\"\n",
    "            # LSTM forward pass\n",
    "            lstm_out, (hidden, cell) = self.lstm(x)\n",
    "            \n",
    "            # Take output from last time step\n",
    "            last_output = lstm_out[:, -1, :]\n",
    "            \n",
    "            # Apply dropout\n",
    "            last_output = self.dropout(last_output)\n",
    "            \n",
    "            # Fully connected layer\n",
    "            output = self.fc(last_output)\n",
    "            \n",
    "            return output\n",
    "    \n",
    "    \n",
    "    # Create model instance\n",
    "    input_size = X_train.shape[2]\n",
    "    model = PlayerLSTM(\n",
    "        input_size=input_size,\n",
    "        hidden_size=config.HIDDEN_SIZE,\n",
    "        num_layers=config.NUM_LAYERS,\n",
    "        dropout=config.DROPOUT\n",
    "    ).to(config.DEVICE)\n",
    "    \n",
    "    # Print model summary\n",
    "    print(\"üèóÔ∏è  LSTM Model Architecture:\")\n",
    "    print(\"=\"*70)\n",
    "    print(model)\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nüìä Model Statistics:\")\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"   Input size: {input_size}\")\n",
    "    print(f\"   Hidden size: {config.HIDDEN_SIZE}\")\n",
    "    print(f\"   Num layers: {config.NUM_LAYERS}\")\n",
    "    print(f\"   Output size: 2 (x, y)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  PyTorch not available. Please install PyTorch to use LSTM models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training üéì\n",
    "\n",
    "Train the LSTM model with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PYTORCH:\n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_train),\n",
    "        torch.FloatTensor(y_train)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_val),\n",
    "        torch.FloatTensor(y_val)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    \n",
    "    print(\"‚úÖ Data loaders created\")\n",
    "    print(f\"   Train batches: {len(train_loader)}\")\n",
    "    print(f\"   Val batches: {len(val_loader)}\")\n",
    "    print(f\"   Batch size: {config.BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PYTORCH:\n",
    "    # Training loop\n",
    "    print(\"\\nüéì Training LSTM model...\\n\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(config.EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(config.DEVICE)\n",
    "            batch_y = batch_y.to(config.DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(config.DEVICE)\n",
    "                batch_y = batch_y.to(config.DEVICE)\n",
    "                \n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Calculate RMSE\n",
    "        train_rmse = np.sqrt(train_loss)\n",
    "        val_rmse = np.sqrt(val_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1:02d}/{config.EPOCHS} | Train RMSE: {train_rmse:.4f} | Val RMSE: {val_rmse:.4f}\", end='')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), config.OUTPUT_DIR / 'best_lstm_model.pth')\n",
    "            print(\" ‚úì (saved)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\" (patience: {patience_counter}/{config.EARLY_STOPPING_PATIENCE})\")\n",
    "            \n",
    "            if patience_counter >= config.EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\n‚èπÔ∏è  Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n‚úÖ Training complete\")\n",
    "    print(f\"   Best validation RMSE: {np.sqrt(best_val_loss):.4f}\")\n",
    "    print(f\"   Model saved to: {config.OUTPUT_DIR / 'best_lstm_model.pth'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PYTORCH:\n",
    "    # Visualize training curves\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "    train_rmse = [np.sqrt(loss) for loss in train_losses]\n",
    "    val_rmse = [np.sqrt(loss) for loss in val_losses]\n",
    "    \n",
    "    ax.plot(epochs_range, train_rmse, 'b-o', label='Train RMSE', linewidth=2, markersize=6)\n",
    "    ax.plot(epochs_range, val_rmse, 'r-o', label='Validation RMSE', linewidth=2, markersize=6)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('RMSE', fontsize=12)\n",
    "    ax.set_title('LSTM Training Curves', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.OUTPUT_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Training curves visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predictions & Visualization üéØ\n",
    "\n",
    "Generate predictions and visualize trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PYTORCH:\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(config.OUTPUT_DIR / 'best_lstm_model.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"üéØ Generating predictions...\\n\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X_val_tensor = torch.FloatTensor(X_val).to(config.DEVICE)\n",
    "        predictions = model(X_val_tensor).cpu().numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    pred_x = predictions[:, 0]\n",
    "    pred_y = predictions[:, 1]\n",
    "    true_x = y_val[:, 0]\n",
    "    true_y = y_val[:, 1]\n",
    "    \n",
    "    rmse_x = np.sqrt(mean_squared_error(true_x, pred_x))\n",
    "    rmse_y = np.sqrt(mean_squared_error(true_y, pred_y))\n",
    "    mae_x = mean_absolute_error(true_x, pred_x)\n",
    "    mae_y = mean_absolute_error(true_y, pred_y)\n",
    "    r2_x = r2_score(true_x, pred_x)\n",
    "    r2_y = r2_score(true_y, pred_y)\n",
    "    \n",
    "    print(\"üìä LSTM Model Performance:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   X Coordinate:\")\n",
    "    print(f\"      RMSE: {rmse_x:.4f}\")\n",
    "    print(f\"      MAE:  {mae_x:.4f}\")\n",
    "    print(f\"      R¬≤:   {r2_x:.4f}\")\n",
    "    print(f\"\\n   Y Coordinate:\")\n",
    "    print(f\"      RMSE: {rmse_y:.4f}\")\n",
    "    print(f\"      MAE:  {mae_y:.4f}\")\n",
    "    print(f\"      R¬≤:   {r2_y:.4f}\")\n",
    "    print(f\"\\n   Average RMSE: {(rmse_x + rmse_y) / 2:.4f}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PYTORCH:\n",
    "    # Visualize predictions\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. X predictions vs actual\n",
    "    axes[0, 0].scatter(true_x, pred_x, alpha=0.3, s=1, c='blue')\n",
    "    axes[0, 0].plot([true_x.min(), true_x.max()], [true_x.min(), true_x.max()], 'r--', lw=2)\n",
    "    axes[0, 0].set_xlabel('Actual X', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Predicted X', fontsize=12)\n",
    "    axes[0, 0].set_title(f'X Predictions (RMSE: {rmse_x:.4f})', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Y predictions vs actual\n",
    "    axes[0, 1].scatter(true_y, pred_y, alpha=0.3, s=1, c='orange')\n",
    "    axes[0, 1].plot([true_y.min(), true_y.max()], [true_y.min(), true_y.max()], 'r--', lw=2)\n",
    "    axes[0, 1].set_xlabel('Actual Y', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Predicted Y', fontsize=12)\n",
    "    axes[0, 1].set_title(f'Y Predictions (RMSE: {rmse_y:.4f})', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. X residuals\n",
    "    residuals_x = true_x - pred_x\n",
    "    axes[1, 0].hist(residuals_x, bins=50, edgecolor='black', alpha=0.7, color='blue')\n",
    "    axes[1, 0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Residual (Actual - Predicted)', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[1, 0].set_title(f'X Residual Distribution (Mean: {residuals_x.mean():.4f})', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 4. Y residuals\n",
    "    residuals_y = true_y - pred_y\n",
    "    axes[1, 1].hist(residuals_y, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[1, 1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Residual (Actual - Predicted)', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[1, 1].set_title(f'Y Residual Distribution (Mean: {residuals_y.mean():.4f})', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.OUTPUT_DIR / 'lstm_predictions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Predictions visualized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PYTORCH:\n",
    "    # Visualize sample trajectories\n",
    "    print(\"\\nüõ§Ô∏è  Visualizing sample trajectories...\\n\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Select 6 random samples\n",
    "    n_samples = 6\n",
    "    sample_indices = np.random.choice(len(X_val), n_samples, replace=False)\n",
    "    \n",
    "    for idx, sample_idx in enumerate(sample_indices):\n",
    "        # Get sequence (historical trajectory)\n",
    "        sequence = X_val[sample_idx]  # Shape: (seq_len, n_features)\n",
    "        \n",
    "        # Denormalize sequence (inverse transform)\n",
    "        sequence_denorm = scaler.inverse_transform(sequence)\n",
    "        \n",
    "        # Extract x, y coordinates (first 2 features)\n",
    "        x_history = sequence_denorm[:, 0]\n",
    "        y_history = sequence_denorm[:, 1]\n",
    "        \n",
    "        # True and predicted future positions\n",
    "        x_true = true_x[sample_idx]\n",
    "        y_true = true_y[sample_idx]\n",
    "        x_pred = pred_x[sample_idx]\n",
    "        y_pred = pred_y[sample_idx]\n",
    "        \n",
    "        # Plot\n",
    "        axes[idx].plot(x_history, y_history, 'b-o', label='Historical trajectory', linewidth=2, markersize=4)\n",
    "        axes[idx].plot(x_true, y_true, 'go', label='Actual future', markersize=12)\n",
    "        axes[idx].plot(x_pred, y_pred, 'r^', label='Predicted future', markersize=12)\n",
    "        axes[idx].plot([x_history[-1], x_true], [y_history[-1], y_true], 'g--', alpha=0.5, linewidth=1)\n",
    "        axes[idx].plot([x_history[-1], x_pred], [y_history[-1], y_pred], 'r--', alpha=0.5, linewidth=1)\n",
    "        \n",
    "        error = np.sqrt((x_true - x_pred)**2 + (y_true - y_pred)**2)\n",
    "        axes[idx].set_xlabel('X Position (yards)', fontsize=10)\n",
    "        axes[idx].set_ylabel('Y Position (yards)', fontsize=10)\n",
    "        axes[idx].set_title(f'Sample {idx+1} (Error: {error:.2f} yards)', fontsize=11, fontweight='bold')\n",
    "        axes[idx].legend(fontsize=8)\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "        axes[idx].set_xlim(0, 120)\n",
    "        axes[idx].set_ylim(0, 53.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.OUTPUT_DIR / 'sample_trajectories.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Sample trajectories visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison with Traditional Models üìä\n",
    "\n",
    "Compare LSTM performance with traditional machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load comparison from previous notebook (if available)\n",
    "comparison_file = Path('../outputs/model_comparison/model_comparison.csv')\n",
    "\n",
    "if comparison_file.exists():\n",
    "    traditional_comparison = pd.read_csv(comparison_file)\n",
    "    \n",
    "    print(\"üìä Comparing LSTM with traditional models...\\n\")\n",
    "    \n",
    "    # Create comparison\n",
    "    lstm_results = pd.DataFrame([{\n",
    "        'model': 'LSTM',\n",
    "        'val_rmse_x': rmse_x,\n",
    "        'val_rmse_y': rmse_y,\n",
    "        'val_mae_x': mae_x,\n",
    "        'val_mae_y': mae_y,\n",
    "        'val_r2_x': r2_x,\n",
    "        'val_r2_y': r2_y,\n",
    "        'avg_rmse': (rmse_x + rmse_y) / 2\n",
    "    }]) if HAS_PYTORCH else pd.DataFrame()\n",
    "    \n",
    "    if not lstm_results.empty:\n",
    "        # Combine results\n",
    "        all_models = pd.concat([traditional_comparison, lstm_results], ignore_index=True)\n",
    "        all_models = all_models.sort_values('avg_rmse')\n",
    "        \n",
    "        print(\"üèÜ Model Ranking (by average RMSE):\\n\")\n",
    "        display(all_models[['model', 'val_rmse_x', 'val_rmse_y', 'avg_rmse']])\n",
    "        \n",
    "        # Visualize comparison\n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        \n",
    "        models_list = all_models['model'].tolist()\n",
    "        x_pos = np.arange(len(models_list))\n",
    "        \n",
    "        # Highlight LSTM\n",
    "        colors = ['red' if m == 'LSTM' else 'steelblue' for m in models_list]\n",
    "        \n",
    "        ax.bar(x_pos, all_models['avg_rmse'], color=colors, alpha=0.7)\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels([m.upper() for m in models_list], rotation=45, ha='right')\n",
    "        ax.set_ylabel('Average RMSE', fontsize=12)\n",
    "        ax.set_title('Model Comparison: LSTM vs Traditional ML', fontsize=14, fontweight='bold')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(config.OUTPUT_DIR / 'lstm_vs_traditional.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n‚úÖ Comparison visualized\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Traditional model comparison not found. Run 04_model_comparison.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis üìâ\n",
    "\n",
    "Analyze LSTM errors by sequence characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PYTORCH:\n",
    "    print(\"üìâ Analyzing prediction errors...\\n\")\n",
    "    \n",
    "    # Calculate Euclidean error\n",
    "    euclidean_errors = np.sqrt((true_x - pred_x)**2 + (true_y - pred_y)**2)\n",
    "    \n",
    "    print(\"üìä Error Statistics:\")\n",
    "    print(f\"   Mean error: {euclidean_errors.mean():.4f} yards\")\n",
    "    print(f\"   Median error: {np.median(euclidean_errors):.4f} yards\")\n",
    "    print(f\"   Std error: {euclidean_errors.std():.4f} yards\")\n",
    "    print(f\"   Min error: {euclidean_errors.min():.4f} yards\")\n",
    "    print(f\"   Max error: {euclidean_errors.max():.4f} yards\")\n",
    "    print(f\"   95th percentile: {np.percentile(euclidean_errors, 95):.4f} yards\")\n",
    "    \n",
    "    # Visualize error distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # 1. Error histogram\n",
    "    axes[0].hist(euclidean_errors, bins=50, edgecolor='black', alpha=0.7, color='purple')\n",
    "    axes[0].axvline(euclidean_errors.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {euclidean_errors.mean():.2f}')\n",
    "    axes[0].axvline(np.median(euclidean_errors), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(euclidean_errors):.2f}')\n",
    "    axes[0].set_xlabel('Euclidean Error (yards)', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].set_title('LSTM Prediction Error Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Error by percentile\n",
    "    percentiles = np.arange(0, 101, 5)\n",
    "    error_percentiles = np.percentile(euclidean_errors, percentiles)\n",
    "    \n",
    "    axes[1].plot(percentiles, error_percentiles, 'b-o', linewidth=2, markersize=6)\n",
    "    axes[1].axhline(euclidean_errors.mean(), color='red', linestyle='--', linewidth=1, label='Mean')\n",
    "    axes[1].set_xlabel('Percentile', fontsize=12)\n",
    "    axes[1].set_ylabel('Error (yards)', fontsize=12)\n",
    "    axes[1].set_title('Error by Percentile', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.OUTPUT_DIR / 'error_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Error analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PYTORCH:\n",
    "    # Save LSTM results\n",
    "    print(\"\\nüíæ Saving LSTM results...\")\n",
    "    \n",
    "    results = {\n",
    "        'rmse_x': float(rmse_x),\n",
    "        'rmse_y': float(rmse_y),\n",
    "        'mae_x': float(mae_x),\n",
    "        'mae_y': float(mae_y),\n",
    "        'r2_x': float(r2_x),\n",
    "        'r2_y': float(r2_y),\n",
    "        'avg_rmse': float((rmse_x + rmse_y) / 2),\n",
    "        'mean_euclidean_error': float(euclidean_errors.mean()),\n",
    "        'median_euclidean_error': float(np.median(euclidean_errors)),\n",
    "        'sequence_length': config.SEQUENCE_LENGTH,\n",
    "        'hidden_size': config.HIDDEN_SIZE,\n",
    "        'num_layers': config.NUM_LAYERS,\n",
    "        'total_parameters': total_params\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(config.OUTPUT_DIR / 'lstm_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Save scaler\n",
    "    with open(config.OUTPUT_DIR / 'scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Results saved to: {config.OUTPUT_DIR}\")\n",
    "    print(f\"   Model: best_lstm_model.pth\")\n",
    "    print(f\"   Results: lstm_results.json\")\n",
    "    print(f\"   Scaler: scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ LSTM Sequence Modeling Complete!\n",
    "\n",
    "### Summary:\n",
    "\n",
    "‚úÖ **Sequence Creation**: Created temporal sequences from tracking data  \n",
    "‚úÖ **LSTM Architecture**: 2-layer LSTM with dropout for trajectory prediction  \n",
    "‚úÖ **Training**: Trained with early stopping and saved best model  \n",
    "‚úÖ **Predictions**: Generated predictions on validation set  \n",
    "‚úÖ **Comparison**: Compared with traditional ML models  \n",
    "‚úÖ **Error Analysis**: Analyzed prediction errors and distributions  \n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **LSTM Performance**: LSTMs can capture temporal dependencies in player movement\n",
    "2. **Sequence Length**: Window of 10 frames provides good context\n",
    "3. **Training**: Early stopping prevents overfitting\n",
    "4. **Comparison**: LSTM may perform better/worse than traditional models depending on data\n",
    "\n",
    "### When LSTM Works Best:\n",
    "\n",
    "- Long, consistent trajectories\n",
    "- Players with predictable movement patterns\n",
    "- When temporal context is important\n",
    "\n",
    "### When Traditional Models May Win:\n",
    "\n",
    "- Short sequences with limited history\n",
    "- Sudden direction changes\n",
    "- When non-temporal features (position, role) are most important\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Generate final predictions in `06_prediction_and_evaluation.ipynb`\n",
    "2. Experiment with different sequence lengths\n",
    "3. Try bidirectional LSTM or GRU architectures\n",
    "4. Combine LSTM with traditional models in ensemble\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

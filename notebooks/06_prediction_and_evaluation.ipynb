{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ NFL Player Movement - Prediction & Evaluation\n",
    "\n",
    "**Final Predictions and Comprehensive Model Evaluation**\n",
    "\n",
    "This notebook generates predictions on the test set and performs detailed evaluation and error analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table of Contents\n",
    "\n",
    "1. [Setup & Configuration](#1-setup)\n",
    "2. [Load Trained Models](#2-load-models)\n",
    "3. [Load Test Data](#3-test-data)\n",
    "4. [Generate Predictions](#4-predictions)\n",
    "5. [Overall Evaluation](#5-evaluation)\n",
    "6. [Residual Analysis](#6-residuals)\n",
    "7. [Error by Categories](#7-categories)\n",
    "8. [Prediction Visualization](#8-visualization)\n",
    "9. [Export Results](#9-export)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration üîß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy import stats\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    \"\"\"Prediction and evaluation configuration\"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    DATA_DIR = Path('../data/raw/train')\n",
    "    MODEL_DIR = Path('../outputs/model_comparison')\n",
    "    OUTPUT_DIR = Path('../outputs/predictions')\n",
    "    \n",
    "    # Data settings\n",
    "    USE_SAMPLE = True\n",
    "    SAMPLE_SIZE = 50000\n",
    "    MAX_FILES = 2\n",
    "    \n",
    "    # Evaluation settings\n",
    "    CONFIDENCE_LEVEL = 0.95\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "config = Config()\n",
    "config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Model directory: {config.MODEL_DIR}\")\n",
    "print(f\"   Output directory: {config.OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Trained Models üì¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ Loading trained models...\\n\")\n",
    "\n",
    "# Find model files\n",
    "model_x_files = list(config.MODEL_DIR.glob('best_model_x_*.pkl'))\n",
    "model_y_files = list(config.MODEL_DIR.glob('best_model_y_*.pkl'))\n",
    "\n",
    "if model_x_files and model_y_files:\n",
    "    # Load models\n",
    "    with open(model_x_files[0], 'rb') as f:\n",
    "        model_x = pickle.load(f)\n",
    "    \n",
    "    with open(model_y_files[0], 'rb') as f:\n",
    "        model_y = pickle.load(f)\n",
    "    \n",
    "    model_x_name = model_x_files[0].stem.replace('best_model_x_', '')\n",
    "    model_y_name = model_y_files[0].stem.replace('best_model_y_', '')\n",
    "    \n",
    "    print(f\"   ‚úì X model loaded: {model_x_name}\")\n",
    "    print(f\"   ‚úì Y model loaded: {model_y_name}\")\n",
    "    \n",
    "    # Load feature names\n",
    "    feature_file = config.MODEL_DIR / 'feature_names.pkl'\n",
    "    if feature_file.exists():\n",
    "        with open(feature_file, 'rb') as f:\n",
    "            feature_names = pickle.load(f)\n",
    "        print(f\"   ‚úì Feature names loaded: {len(feature_names)} features\")\n",
    "    else:\n",
    "        feature_names = None\n",
    "        print(\"   ‚ö†Ô∏è  Feature names not found\")\n",
    "    \n",
    "    MODELS_LOADED = True\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No trained models found. Run 04_model_comparison.ipynb first.\")\n",
    "    MODELS_LOADED = False\n",
    "\n",
    "print(\"\\n‚úÖ Models loaded\" if MODELS_LOADED else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Test Data üìÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(data_dir, max_files=None, sample_size=None):\n",
    "    \"\"\"\n",
    "    Load and prepare test data\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Loading test data...\\n\")\n",
    "    \n",
    "    # Load files\n",
    "    input_files = sorted(data_dir.glob('input_*.csv'))[:max_files] if max_files else sorted(data_dir.glob('input_*.csv'))\n",
    "    output_files = sorted(data_dir.glob('output_*.csv'))[:max_files] if max_files else sorted(data_dir.glob('output_*.csv'))\n",
    "    \n",
    "    input_df = pd.concat([pd.read_csv(f) for f in input_files], ignore_index=True)\n",
    "    output_df = pd.concat([pd.read_csv(f) for f in output_files], ignore_index=True)\n",
    "    \n",
    "    print(f\"   Input: {input_df.shape}\")\n",
    "    print(f\"   Output: {output_df.shape}\")\n",
    "    \n",
    "    # Sample\n",
    "    if sample_size and len(input_df) > sample_size:\n",
    "        input_df = input_df.sample(n=sample_size, random_state=42)\n",
    "        sampled_keys = input_df[['game_id', 'play_id', 'nfl_id', 'frame_id']]\n",
    "        output_df = output_df.merge(sampled_keys, on=['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    \n",
    "    # Merge\n",
    "    df = input_df.merge(\n",
    "        output_df[['game_id', 'play_id', 'nfl_id', 'frame_id', 'x', 'y']],\n",
    "        on=['game_id', 'play_id', 'nfl_id', 'frame_id'],\n",
    "        suffixes=('', '_target')\n",
    "    )\n",
    "    df = df.rename(columns={'x_target': 'target_x', 'y_target': 'target_y'})\n",
    "    \n",
    "    # Handle missing\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().any():\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Data loaded: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create features for prediction\n",
    "    \"\"\"\n",
    "    print(\"\\n‚öôÔ∏è  Creating features...\\n\")\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Physics features\n",
    "    if 's' in df_copy.columns and 'dir' in df_copy.columns:\n",
    "        df_copy['velocity_x'] = df_copy['s'] * np.cos(np.radians(df_copy['dir']))\n",
    "        df_copy['velocity_y'] = df_copy['s'] * np.sin(np.radians(df_copy['dir']))\n",
    "    \n",
    "    if 'a' in df_copy.columns and 'dir' in df_copy.columns:\n",
    "        df_copy['acceleration_x'] = df_copy['a'] * np.cos(np.radians(df_copy['dir']))\n",
    "        df_copy['acceleration_y'] = df_copy['a'] * np.sin(np.radians(df_copy['dir']))\n",
    "    \n",
    "    if 'player_weight' in df_copy.columns and 's' in df_copy.columns:\n",
    "        df_copy['momentum'] = df_copy['player_weight'] * df_copy['s']\n",
    "        df_copy['kinetic_energy'] = 0.5 * df_copy['player_weight'] * (df_copy['s'] ** 2)\n",
    "    \n",
    "    # Spatial features\n",
    "    if all(col in df_copy.columns for col in ['x', 'y', 'ball_land_x', 'ball_land_y']):\n",
    "        df_copy['dist_to_ball'] = np.sqrt((df_copy['x'] - df_copy['ball_land_x'])**2 + (df_copy['y'] - df_copy['ball_land_y'])**2)\n",
    "        df_copy['dx_to_ball'] = df_copy['ball_land_x'] - df_copy['x']\n",
    "        df_copy['dy_to_ball'] = df_copy['ball_land_y'] - df_copy['y']\n",
    "    \n",
    "    if 'x' in df_copy.columns:\n",
    "        df_copy['field_position_norm'] = df_copy['x'] / 120.0\n",
    "        df_copy['dist_to_sideline'] = np.minimum(df_copy['y'], 53.3 - df_copy['y'])\n",
    "    \n",
    "    # NFL features\n",
    "    if 'player_role' in df_copy.columns:\n",
    "        df_copy['is_targeted_receiver'] = (df_copy['player_role'] == 'Targeted Receiver').astype(int)\n",
    "        df_copy['is_passer'] = (df_copy['player_role'] == 'Passer').astype(int)\n",
    "    \n",
    "    print(f\"   ‚úì Features created: {len([c for c in df_copy.columns if c not in df.columns])} new\")\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "# Load and prepare test data\n",
    "test_df = load_and_prepare_data(\n",
    "    config.DATA_DIR,\n",
    "    max_files=config.MAX_FILES,\n",
    "    sample_size=config.SAMPLE_SIZE if config.USE_SAMPLE else None\n",
    ")\n",
    "\n",
    "test_df = create_features(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Predictions üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODELS_LOADED:\n",
    "    print(\"üéØ Generating predictions...\\n\")\n",
    "    \n",
    "    # Prepare features\n",
    "    exclude_cols = ['game_id', 'play_id', 'nfl_id', 'frame_id', 'target_x', 'target_y',\n",
    "                    'player_name', 'player_position', 'player_role', 'player_side',\n",
    "                    'play_direction', 'player_birth_date', 'player_to_predict']\n",
    "    \n",
    "    if feature_names:\n",
    "        X_test = test_df[feature_names].fillna(0)\n",
    "    else:\n",
    "        feature_cols = [col for col in test_df.columns if col not in exclude_cols and test_df[col].dtype in ['int64', 'float64']]\n",
    "        X_test = test_df[feature_cols].fillna(0)\n",
    "    \n",
    "    y_test_x = test_df['target_x'].fillna(test_df['target_x'].median())\n",
    "    y_test_y = test_df['target_y'].fillna(test_df['target_y'].median())\n",
    "    \n",
    "    print(f\"   Features shape: {X_test.shape}\")\n",
    "    print(f\"   Targets shape: ({len(y_test_x)}, 2)\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    pred_x = model_x.predict(X_test)\n",
    "    pred_y = model_y.predict(X_test)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Predictions generated\")\n",
    "    print(f\"   X predictions: {len(pred_x):,}\")\n",
    "    print(f\"   Y predictions: {len(pred_y):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Overall Evaluation üìä\n",
    "\n",
    "Calculate comprehensive performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODELS_LOADED:\n",
    "    print(\"=\"*70)\n",
    "    print(\"OVERALL MODEL PERFORMANCE\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'X Coordinate': {\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test_x, pred_x)),\n",
    "            'MAE': mean_absolute_error(y_test_x, pred_x),\n",
    "            'R¬≤': r2_score(y_test_x, pred_x),\n",
    "            'MAPE': np.mean(np.abs((y_test_x - pred_x) / y_test_x)) * 100\n",
    "        },\n",
    "        'Y Coordinate': {\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test_y, pred_y)),\n",
    "            'MAE': mean_absolute_error(y_test_y, pred_y),\n",
    "            'R¬≤': r2_score(y_test_y, pred_y),\n",
    "            'MAPE': np.mean(np.abs((y_test_y - pred_y) / y_test_y)) * 100\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Euclidean error\n",
    "    euclidean_error = np.sqrt((y_test_x - pred_x)**2 + (y_test_y - pred_y)**2)\n",
    "    \n",
    "    # Display metrics\n",
    "    for coord, vals in metrics.items():\n",
    "        print(f\"üìä {coord}:\")\n",
    "        for metric, value in vals.items():\n",
    "            print(f\"   {metric:6s}: {value:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"üìè Euclidean Distance Error:\")\n",
    "    print(f\"   Mean:   {euclidean_error.mean():.4f} yards\")\n",
    "    print(f\"   Median: {np.median(euclidean_error):.4f} yards\")\n",
    "    print(f\"   Std:    {euclidean_error.std():.4f} yards\")\n",
    "    print(f\"   Max:    {euclidean_error.max():.4f} yards\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    # Create metrics dataframe\n",
    "    metrics_df = pd.DataFrame(metrics).T\n",
    "    metrics_df.to_csv(config.OUTPUT_DIR / 'overall_metrics.csv')\n",
    "    print(f\"\\n‚úÖ Metrics saved to: {config.OUTPUT_DIR / 'overall_metrics.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Residual Analysis üìâ\n",
    "\n",
    "Analyze prediction residuals for model diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODELS_LOADED:\n",
    "    print(\"üìâ Analyzing residuals...\\n\")\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals_x = y_test_x - pred_x\n",
    "    residuals_y = y_test_y - pred_y\n",
    "    \n",
    "    # Visualize residuals\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Row 1: X coordinate\n",
    "    # 1. Residuals vs Predicted\n",
    "    axes[0, 0].scatter(pred_x, residuals_x, alpha=0.3, s=1, c='blue')\n",
    "    axes[0, 0].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Predicted X', fontsize=11)\n",
    "    axes[0, 0].set_ylabel('Residuals', fontsize=11)\n",
    "    axes[0, 0].set_title('X: Residuals vs Predicted', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Histogram\n",
    "    axes[0, 1].hist(residuals_x, bins=50, edgecolor='black', alpha=0.7, color='blue')\n",
    "    axes[0, 1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0, 1].axvline(residuals_x.mean(), color='green', linestyle='--', linewidth=2, label=f'Mean: {residuals_x.mean():.3f}')\n",
    "    axes[0, 1].set_xlabel('Residuals', fontsize=11)\n",
    "    axes[0, 1].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[0, 1].set_title('X: Residual Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. Q-Q Plot\n",
    "    stats.probplot(residuals_x, dist=\"norm\", plot=axes[0, 2])\n",
    "    axes[0, 2].set_title('X: Q-Q Plot', fontsize=12, fontweight='bold')\n",
    "    axes[0, 2].grid(alpha=0.3)\n",
    "    \n",
    "    # Row 2: Y coordinate\n",
    "    # 4. Residuals vs Predicted\n",
    "    axes[1, 0].scatter(pred_y, residuals_y, alpha=0.3, s=1, c='orange')\n",
    "    axes[1, 0].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Predicted Y', fontsize=11)\n",
    "    axes[1, 0].set_ylabel('Residuals', fontsize=11)\n",
    "    axes[1, 0].set_title('Y: Residuals vs Predicted', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # 5. Histogram\n",
    "    axes[1, 1].hist(residuals_y, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[1, 1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[1, 1].axvline(residuals_y.mean(), color='green', linestyle='--', linewidth=2, label=f'Mean: {residuals_y.mean():.3f}')\n",
    "    axes[1, 1].set_xlabel('Residuals', fontsize=11)\n",
    "    axes[1, 1].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[1, 1].set_title('Y: Residual Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    # 6. Q-Q Plot\n",
    "    stats.probplot(residuals_y, dist=\"norm\", plot=axes[1, 2])\n",
    "    axes[1, 2].set_title('Y: Q-Q Plot', fontsize=12, fontweight='bold')\n",
    "    axes[1, 2].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.OUTPUT_DIR / 'residual_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Residual analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error by Categories üè∑Ô∏è\n",
    "\n",
    "Analyze errors by player position, field region, speed, and distance to ball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODELS_LOADED:\n",
    "    # Add predictions to dataframe\n",
    "    test_df['pred_x'] = pred_x\n",
    "    test_df['pred_y'] = pred_y\n",
    "    test_df['error_x'] = np.abs(y_test_x - pred_x)\n",
    "    test_df['error_y'] = np.abs(y_test_y - pred_y)\n",
    "    test_df['euclidean_error'] = euclidean_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODELS_LOADED:\n",
    "    print(\"üè∑Ô∏è  Analyzing errors by categories...\\n\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Error by Player Position\n",
    "    if 'player_position' in test_df.columns:\n",
    "        pos_errors = test_df.groupby('player_position')['euclidean_error'].agg(['mean', 'median', 'count'])\n",
    "        pos_errors = pos_errors[pos_errors['count'] >= 50].sort_values('mean', ascending=False).head(10)\n",
    "        \n",
    "        axes[0, 0].barh(range(len(pos_errors)), pos_errors['mean'], alpha=0.7, color='steelblue')\n",
    "        axes[0, 0].set_yticks(range(len(pos_errors)))\n",
    "        axes[0, 0].set_yticklabels(pos_errors.index)\n",
    "        axes[0, 0].set_xlabel('Mean Euclidean Error (yards)', fontsize=11)\n",
    "        axes[0, 0].set_title('Error by Player Position (Top 10)', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        print(\"üìä Error by Position (Top 10):\")\n",
    "        display(pos_errors.head(10))\n",
    "    \n",
    "    # 2. Error by Field Region\n",
    "    if 'x' in test_df.columns:\n",
    "        test_df['field_region'] = pd.cut(test_df['x'], bins=[0, 20, 40, 60, 80, 100, 120],\n",
    "                                         labels=['0-20', '20-40', '40-60', '60-80', '80-100', '100-120'])\n",
    "        region_errors = test_df.groupby('field_region')['euclidean_error'].mean()\n",
    "        \n",
    "        axes[0, 1].bar(range(len(region_errors)), region_errors.values, alpha=0.7, color='orange')\n",
    "        axes[0, 1].set_xticks(range(len(region_errors)))\n",
    "        axes[0, 1].set_xticklabels(region_errors.index, rotation=45)\n",
    "        axes[0, 1].set_ylabel('Mean Euclidean Error (yards)', fontsize=11)\n",
    "        axes[0, 1].set_xlabel('Field Region (X yards)', fontsize=11)\n",
    "        axes[0, 1].set_title('Error by Field Region', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. Error by Speed Range\n",
    "    if 's' in test_df.columns:\n",
    "        test_df['speed_range'] = pd.cut(test_df['s'], bins=[0, 2, 4, 6, 8, 100],\n",
    "                                        labels=['0-2', '2-4', '4-6', '6-8', '8+'])\n",
    "        speed_errors = test_df.groupby('speed_range')['euclidean_error'].mean()\n",
    "        \n",
    "        axes[1, 0].bar(range(len(speed_errors)), speed_errors.values, alpha=0.7, color='green')\n",
    "        axes[1, 0].set_xticks(range(len(speed_errors)))\n",
    "        axes[1, 0].set_xticklabels(speed_errors.index)\n",
    "        axes[1, 0].set_ylabel('Mean Euclidean Error (yards)', fontsize=11)\n",
    "        axes[1, 0].set_xlabel('Speed Range (yards/sec)', fontsize=11)\n",
    "        axes[1, 0].set_title('Error by Speed Range', fontsize=12, fontweight='bold')\n",
    "        axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. Error by Distance to Ball\n",
    "    if 'dist_to_ball' in test_df.columns:\n",
    "        test_df['dist_to_ball_range'] = pd.cut(test_df['dist_to_ball'], bins=[0, 10, 20, 30, 40, 1000],\n",
    "                                               labels=['0-10', '10-20', '20-30', '30-40', '40+'])\n",
    "        ball_errors = test_df.groupby('dist_to_ball_range')['euclidean_error'].mean()\n",
    "        \n",
    "        axes[1, 1].bar(range(len(ball_errors)), ball_errors.values, alpha=0.7, color='purple')\n",
    "        axes[1, 1].set_xticks(range(len(ball_errors)))\n",
    "        axes[1, 1].set_xticklabels(ball_errors.index)\n",
    "        axes[1, 1].set_ylabel('Mean Euclidean Error (yards)', fontsize=11)\n",
    "        axes[1, 1].set_xlabel('Distance to Ball (yards)', fontsize=11)\n",
    "        axes[1, 1].set_title('Error by Distance to Ball', fontsize=12, fontweight='bold')\n",
    "        axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.OUTPUT_DIR / 'error_by_categories.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Category analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prediction Visualization üé®\n",
    "\n",
    "Visualize predictions on the field and compare with actual positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODELS_LOADED:\n",
    "    print(\"üé® Visualizing predictions...\\n\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Predicted vs Actual (X)\n",
    "    axes[0, 0].scatter(y_test_x, pred_x, alpha=0.3, s=1, c='blue')\n",
    "    axes[0, 0].plot([y_test_x.min(), y_test_x.max()], [y_test_x.min(), y_test_x.max()], 'r--', lw=2)\n",
    "    axes[0, 0].set_xlabel('Actual X', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Predicted X', fontsize=12)\n",
    "    axes[0, 0].set_title(f'X Coordinate: Predicted vs Actual\\n(R¬≤ = {metrics[\"X Coordinate\"][\"R¬≤\"]:.4f})', \n",
    "                         fontsize=13, fontweight='bold')\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Predicted vs Actual (Y)\n",
    "    axes[0, 1].scatter(y_test_y, pred_y, alpha=0.3, s=1, c='orange')\n",
    "    axes[0, 1].plot([y_test_y.min(), y_test_y.max()], [y_test_y.min(), y_test_y.max()], 'r--', lw=2)\n",
    "    axes[0, 1].set_xlabel('Actual Y', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Predicted Y', fontsize=12)\n",
    "    axes[0, 1].set_title(f'Y Coordinate: Predicted vs Actual\\n(R¬≤ = {metrics[\"Y Coordinate\"][\"R¬≤\"]:.4f})', \n",
    "                         fontsize=13, fontweight='bold')\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Error heatmap on field\n",
    "    sample = test_df.sample(min(5000, len(test_df)))\n",
    "    scatter = axes[1, 0].scatter(sample['x'], sample['y'], c=sample['euclidean_error'], \n",
    "                                 cmap='RdYlGn_r', alpha=0.5, s=10, vmin=0, vmax=np.percentile(euclidean_error, 95))\n",
    "    axes[1, 0].set_xlabel('X Position (yards)', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Y Position (yards)', fontsize=12)\n",
    "    axes[1, 0].set_title('Error Heatmap on Field', fontsize=13, fontweight='bold')\n",
    "    axes[1, 0].set_xlim(0, 120)\n",
    "    axes[1, 0].set_ylim(0, 53.3)\n",
    "    plt.colorbar(scatter, ax=axes[1, 0], label='Error (yards)')\n",
    "    \n",
    "    # 4. Error distribution (Euclidean)\n",
    "    axes[1, 1].hist(euclidean_error, bins=50, edgecolor='black', alpha=0.7, color='purple')\n",
    "    axes[1, 1].axvline(euclidean_error.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                       label=f'Mean: {euclidean_error.mean():.2f}')\n",
    "    axes[1, 1].axvline(np.median(euclidean_error), color='green', linestyle='--', linewidth=2, \n",
    "                       label=f'Median: {np.median(euclidean_error):.2f}')\n",
    "    axes[1, 1].set_xlabel('Euclidean Error (yards)', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[1, 1].set_title('Euclidean Distance Error Distribution', fontsize=13, fontweight='bold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.OUTPUT_DIR / 'prediction_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Predictions visualized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODELS_LOADED:\n",
    "    # Visualize sample play trajectories\n",
    "    print(\"\\nüèà Visualizing sample play trajectories...\\n\")\n",
    "    \n",
    "    # Select a random play with multiple players\n",
    "    if 'play_id' in test_df.columns:\n",
    "        play_counts = test_df.groupby('play_id').size()\n",
    "        sample_play = play_counts[play_counts >= 10].sample(1, random_state=42).index[0]\n",
    "        play_data = test_df[test_df['play_id'] == sample_play]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "        \n",
    "        # Plot field boundaries\n",
    "        ax.plot([0, 120], [0, 0], 'k-', linewidth=2)\n",
    "        ax.plot([0, 120], [53.3, 53.3], 'k-', linewidth=2)\n",
    "        ax.plot([0, 0], [0, 53.3], 'k-', linewidth=2)\n",
    "        ax.plot([120, 120], [0, 53.3], 'k-', linewidth=2)\n",
    "        \n",
    "        # Plot yard lines\n",
    "        for yard in range(10, 120, 10):\n",
    "            ax.plot([yard, yard], [0, 53.3], 'k-', linewidth=0.5, alpha=0.3)\n",
    "        \n",
    "        # Plot players\n",
    "        for idx, row in play_data.iterrows():\n",
    "            # Current position\n",
    "            ax.plot(row['x'], row['y'], 'bo', markersize=8, alpha=0.7)\n",
    "            \n",
    "            # Actual future position\n",
    "            ax.plot(row['target_x'], row['target_y'], 'go', markersize=10, alpha=0.7)\n",
    "            \n",
    "            # Predicted future position\n",
    "            ax.plot(row['pred_x'], row['pred_y'], 'r^', markersize=10, alpha=0.7)\n",
    "            \n",
    "            # Lines\n",
    "            ax.plot([row['x'], row['target_x']], [row['y'], row['target_y']], 'g--', alpha=0.3, linewidth=1)\n",
    "            ax.plot([row['x'], row['pred_x']], [row['y'], row['pred_y']], 'r--', alpha=0.3, linewidth=1)\n",
    "        \n",
    "        # Legend\n",
    "        ax.plot([], [], 'bo', markersize=8, label='Current Position')\n",
    "        ax.plot([], [], 'go', markersize=10, label='Actual Future')\n",
    "        ax.plot([], [], 'r^', markersize=10, label='Predicted Future')\n",
    "        \n",
    "        ax.set_xlabel('X Position (yards)', fontsize=12)\n",
    "        ax.set_ylabel('Y Position (yards)', fontsize=12)\n",
    "        ax.set_title(f'Sample Play Trajectories (Play ID: {sample_play})', fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=11, loc='upper right')\n",
    "        ax.set_xlim(-5, 125)\n",
    "        ax.set_ylim(-5, 58)\n",
    "        ax.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(config.OUTPUT_DIR / 'sample_play_trajectories.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Sample play visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results üíæ\n",
    "\n",
    "Export predictions and create submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODELS_LOADED:\n",
    "    print(\"üíæ Exporting results...\\n\")\n",
    "    \n",
    "    # Create submission file\n",
    "    submission = test_df[['game_id', 'play_id', 'nfl_id', 'frame_id']].copy()\n",
    "    submission['x'] = pred_x\n",
    "    submission['y'] = pred_y\n",
    "    \n",
    "    submission_file = config.OUTPUT_DIR / 'submission.csv'\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    print(f\"   ‚úì Submission saved: {submission_file}\")\n",
    "    print(f\"      Shape: {submission.shape}\")\n",
    "    \n",
    "    # Save detailed predictions with errors\n",
    "    detailed_predictions = test_df[['game_id', 'play_id', 'nfl_id', 'frame_id', \n",
    "                                     'x', 'y', 'target_x', 'target_y', \n",
    "                                     'pred_x', 'pred_y', 'error_x', 'error_y', 'euclidean_error']].copy()\n",
    "    \n",
    "    detailed_file = config.OUTPUT_DIR / 'detailed_predictions.csv'\n",
    "    detailed_predictions.to_csv(detailed_file, index=False)\n",
    "    print(f\"   ‚úì Detailed predictions saved: {detailed_file}\")\n",
    "    \n",
    "    # Save error summary by categories\n",
    "    error_summary = {\n",
    "        'by_position': test_df.groupby('player_position')['euclidean_error'].mean().to_dict() if 'player_position' in test_df.columns else {},\n",
    "        'by_field_region': test_df.groupby('field_region')['euclidean_error'].mean().to_dict() if 'field_region' in test_df.columns else {},\n",
    "        'by_speed_range': test_df.groupby('speed_range')['euclidean_error'].mean().to_dict() if 'speed_range' in test_df.columns else {},\n",
    "        'by_distance_to_ball': test_df.groupby('dist_to_ball_range')['euclidean_error'].mean().to_dict() if 'dist_to_ball_range' in test_df.columns else {}\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(config.OUTPUT_DIR / 'error_summary.json', 'w') as f:\n",
    "        json.dump(error_summary, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"   ‚úì Error summary saved: {config.OUTPUT_DIR / 'error_summary.json'}\")\n",
    "    \n",
    "    # Generate final report\n",
    "    report = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'model_x': model_x_name,\n",
    "        'model_y': model_y_name,\n",
    "        'num_predictions': len(submission),\n",
    "        'metrics': {\n",
    "            'x_rmse': float(metrics['X Coordinate']['RMSE']),\n",
    "            'y_rmse': float(metrics['Y Coordinate']['RMSE']),\n",
    "            'x_mae': float(metrics['X Coordinate']['MAE']),\n",
    "            'y_mae': float(metrics['Y Coordinate']['MAE']),\n",
    "            'x_r2': float(metrics['X Coordinate']['R¬≤']),\n",
    "            'y_r2': float(metrics['Y Coordinate']['R¬≤']),\n",
    "            'mean_euclidean_error': float(euclidean_error.mean()),\n",
    "            'median_euclidean_error': float(np.median(euclidean_error))\n",
    "        },\n",
    "        'files': {\n",
    "            'submission': str(submission_file),\n",
    "            'detailed_predictions': str(detailed_file),\n",
    "            'error_summary': str(config.OUTPUT_DIR / 'error_summary.json')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(config.OUTPUT_DIR / 'final_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    print(f\"   ‚úì Final report saved: {config.OUTPUT_DIR / 'final_report.json'}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ All results exported to: {config.OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODELS_LOADED:\n",
    "    # Display final summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    print(f\"üìä Models Used:\")\n",
    "    print(f\"   X coordinate: {model_x_name}\")\n",
    "    print(f\"   Y coordinate: {model_y_name}\")\n",
    "    \n",
    "    print(f\"\\nüìà Performance:\")\n",
    "    print(f\"   Average RMSE: {(metrics['X Coordinate']['RMSE'] + metrics['Y Coordinate']['RMSE']) / 2:.4f} yards\")\n",
    "    print(f\"   Mean Euclidean Error: {euclidean_error.mean():.4f} yards\")\n",
    "    print(f\"   Median Euclidean Error: {np.median(euclidean_error):.4f} yards\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Output Files:\")\n",
    "    print(f\"   Submission: {submission_file}\")\n",
    "    print(f\"   Detailed predictions: {detailed_file}\")\n",
    "    print(f\"   Final report: {config.OUTPUT_DIR / 'final_report.json'}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Next Steps:\")\n",
    "    print(f\"   1. Review error analysis for improvement opportunities\")\n",
    "    print(f\"   2. Consider ensemble methods to combine multiple models\")\n",
    "    print(f\"   3. Experiment with feature engineering\")\n",
    "    print(f\"   4. Submit predictions to competition (if applicable)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Prediction & Evaluation Complete!\n",
    "\n",
    "### Summary:\n",
    "\n",
    "‚úÖ **Models Loaded**: Best performing models from model comparison  \n",
    "‚úÖ **Predictions Generated**: Future player positions predicted  \n",
    "‚úÖ **Comprehensive Evaluation**: RMSE, MAE, R¬≤, MAPE calculated  \n",
    "‚úÖ **Residual Analysis**: Diagnostic plots for model quality  \n",
    "‚úÖ **Category Analysis**: Errors by position, region, speed, distance  \n",
    "‚úÖ **Visualizations**: Field heatmaps, trajectories, error distributions  \n",
    "‚úÖ **Results Exported**: Submission file and detailed analysis  \n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Model Performance**: Check RMSE and R¬≤ scores above\n",
    "2. **Error Patterns**: Some positions/regions harder to predict\n",
    "3. **Residuals**: Check Q-Q plots for normality assumption\n",
    "4. **Field Coverage**: Errors may vary by field location\n",
    "\n",
    "### Files Created:\n",
    "\n",
    "- `submission.csv`: Competition submission file\n",
    "- `detailed_predictions.csv`: Predictions with errors\n",
    "- `final_report.json`: Summary of results\n",
    "- `overall_metrics.csv`: Performance metrics\n",
    "- `error_summary.json`: Errors by categories\n",
    "- Multiple visualization PNGs\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **High Error Positions**: Focus on improving predictions for difficult positions\n",
    "2. **Field Regions**: Consider region-specific models\n",
    "3. **Speed Ranges**: Different models for different speed ranges\n",
    "4. **Ensemble**: Combine multiple models for better performance\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've completed the full NFL Player Movement Prediction pipeline!** üèàüéâ\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
